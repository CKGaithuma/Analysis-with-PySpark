{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Context\n",
    "\n",
    "sc = SparkContext(master=\"local\", \n",
    "                         appName=\"Low Level API\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Spark Context information\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD from a list\n",
    "# Main way is using parallelize method\n",
    "\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10],\n",
    "                     numSlices=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the RDD variable is not the same as\n",
    "# calling the RDD values\n",
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE OF TRANSFORMATIONS ON RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to view any results stored within an RDD after creating one, we must use the collect() method. This method will return all the elements of the RDD as an array at the driver program. This is usually useful when we have a small dataset that we want to view. However, if we have a large dataset, we can use the take() method to return a small number of elements from the RDD. The take() method returns an array of the first n elements of the RDD at the driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print values from rdd using take action\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the filter action together with a lambda function to filter out elements from an RDD. The filter action takes in a lambda function that returns a boolean value. If the lambda function returns true, the element will be kept in the RDD. If the lambda function returns false, the element will be filtered out of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out even numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sample action allows you to take a sample of the RDD. The sample action takes in three parameters. The first parameter is whether the sampling is done with replacement or not. The second parameter is the sample size as a fraction. The third parameter is the random seed. The sample action returns an RDD with the sampled elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map action allows you to apply a function to each element in the RDD. The map action takes in a lambda function that returns a new value for each element in the RDD. The map action returns an RDD with the new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.map(lambda x: (x, str(x))).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to pay attention to the difference between Map and flatMap. The map action takes in a lambda function that returns a single element. The flatMap action takes in a lambda function that returns an iterable object. The flatMap action returns an RDD with the elements from all the iterable objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.flatMap(lambda x: (x, str(x))).take(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE USE OF RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a text file and save it to an RDD called lyrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 5 lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all words to lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the lines into words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each word to a tuple (word, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce by key to count the number of times each word appears\n",
    "# Here is an example to show the output of reduceByKey\n",
    "# Uncomment the code to preview the output\n",
    "# wordCount.reduceByKey(lambda x,y: x+y).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** You have to understand how this transformation works. It can be quite tricky.\n",
    "\n",
    "reduceByKey works by applying the function provided in the first parameter to all the values that have the same key. The function provided in the first parameter must be commutative and associative. The reduceByKey action returns an RDD with the same key and a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the reduceByKey output to a new variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is also possible to take elements in an ordered way\n",
    "# The negative sign is to sort in descending order\n",
    "# wordCount.takeOrdered(10, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the record, it is possible to chain all of the steps we took right before saving the RDD into a single line of code. However, it is not recommended to do so. It is better to break down the steps into multiple lines of code for readability purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of word count using chaining transformations\n",
    "\n",
    "(sc.textFile('./tmntLyrics.txt') # Read text file \n",
    " .map(lambda x: x.lower()) # Lowercase conversion\n",
    " .flatMap(lambda x: x.split())  # Split lines into words\n",
    " .map(lambda x: (x, 1)) # Map each word to a tuple (word, 1)\n",
    " .reduceByKey(lambda x,y: x+y)  # Reduce by key to count\n",
    " .takeOrdered(10, key=lambda x: -x[1])) # Take top 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the result to a text file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark Context\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(spark-env)",
   "language": "python",
   "name": "spark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
